{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.11.3-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.11.4 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.11.4-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
      "Collecting trl!=0.19.0,<=0.23.0,>=0.18.2 (from unsloth)\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.4->unsloth)\n",
      "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.11.4->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.11.4->unsloth)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
      "Downloading unsloth-2025.11.3-py3-none-any.whl (353 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m353.0/353.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.11.4-py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, shtab, pyarrow, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.19.0 pyarrow-22.0.0 shtab-1.8.0 torchao-0.14.1 trl-0.23.0 tyro-0.9.35 unsloth-2025.11.3 unsloth_zoo-2025.11.4 xformers-0.0.33.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Processing file 13jVkZtY9-AIu5Wm9CrOApKROCTuWAS5R adapter_config.json\n",
      "Processing file 1AF0J0PkncFLAnqfeROpDEFO6kdnbVDTM adapter_model.safetensors\n",
      "Processing file 14uiKATXyCdwuLUxxLuGzUMjWR0t7D9FT added_tokens.json\n",
      "Processing file 12gMPExwLPcJpID7vCOdA7Tkcl95qxky3 chat_template.jinja\n",
      "Processing file 1VmcoZaB65E4iIqucPRkDZb8kW2Y2yepo merges.txt\n",
      "Processing file 1EnhklNi4W6T6jAjcp6gjs6uvq997KQrF README.md\n",
      "Processing file 1gvAK4AcYOfV19FXmUiHLfzBAOxinsgfj special_tokens_map.json\n",
      "Processing file 1QhsvdoEqqXlPfhHHzyvGhj6AJA0dQFAv tokenizer_config.json\n",
      "Processing file 11sdQDHMdMgnz4dy_LIw2dowYWGrbppzT tokenizer.json\n",
      "Processing file 1_-Ni-zffQLwVwL4ghgFF8Sl8_5m1hAa3 vocab.json\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=13jVkZtY9-AIu5Wm9CrOApKROCTuWAS5R\n",
      "To: /content/lora_model/adapter_config.json\n",
      "100% 1.08k/1.08k [00:00<00:00, 4.95MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1AF0J0PkncFLAnqfeROpDEFO6kdnbVDTM\n",
      "From (redirected): https://drive.google.com/uc?id=1AF0J0PkncFLAnqfeROpDEFO6kdnbVDTM&confirm=t&uuid=5a3a93f1-3489-477a-b656-6a70de986bf8\n",
      "To: /content/lora_model/adapter_model.safetensors\n",
      "100% 264M/264M [00:03<00:00, 77.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14uiKATXyCdwuLUxxLuGzUMjWR0t7D9FT\n",
      "To: /content/lora_model/added_tokens.json\n",
      "100% 707/707 [00:00<00:00, 3.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12gMPExwLPcJpID7vCOdA7Tkcl95qxky3\n",
      "To: /content/lora_model/chat_template.jinja\n",
      "100% 4.67k/4.67k [00:00<00:00, 19.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VmcoZaB65E4iIqucPRkDZb8kW2Y2yepo\n",
      "To: /content/lora_model/merges.txt\n",
      "100% 1.67M/1.67M [00:00<00:00, 112MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EnhklNi4W6T6jAjcp6gjs6uvq997KQrF\n",
      "To: /content/lora_model/README.md\n",
      "100% 5.24k/5.24k [00:00<00:00, 20.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1gvAK4AcYOfV19FXmUiHLfzBAOxinsgfj\n",
      "To: /content/lora_model/special_tokens_map.json\n",
      "100% 614/614 [00:00<00:00, 665kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1QhsvdoEqqXlPfhHHzyvGhj6AJA0dQFAv\n",
      "To: /content/lora_model/tokenizer_config.json\n",
      "100% 5.43k/5.43k [00:00<00:00, 19.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11sdQDHMdMgnz4dy_LIw2dowYWGrbppzT\n",
      "To: /content/lora_model/tokenizer.json\n",
      "100% 11.4M/11.4M [00:00<00:00, 86.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_-Ni-zffQLwVwL4ghgFF8Sl8_5m1hAa3\n",
      "To: /content/lora_model/vocab.json\n",
      "100% 2.78M/2.78M [00:00<00:00, 53.3MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder https://drive.google.com/drive/u/0/folders/1arl5hzp6La-dibY1kXQ9zPkInraTkPDb -O ./lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Qwen3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1492221d744fab40842e1b87d22f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66ee72a409045e497e6d026d070d385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./lora_model\",\n",
    "    # model_name = \"/content/drive/MyDrive/lora_model\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat started! Type 'quit' to exit, or 'save_name:YourName' to save your name.\n",
      "\n",
      "You: quit\n",
      "Chat ended\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MEMORY_FILE = Path(\"chat_memory.json\")\n",
    "if MEMORY_FILE.exists():\n",
    "    long_memory = json.loads(MEMORY_FILE.read_text())\n",
    "else:\n",
    "    long_memory = {\"name\": None, \"facts\": []}\n",
    "\n",
    "\n",
    "def clean_generated(text: str) -> str:\n",
    "    \"\"\"Remove unwanted tags or prefixes.\"\"\"\n",
    "    text = text.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "    text = text.replace(\"Assistant:\", \"\").replace(\"User:\", \"\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def prune_messages(tokenizer, messages, reserve_new_tokens=512):\n",
    "    \"\"\"Keep conversation within the model's token limit.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    tokens = tokenizer.encode(text)\n",
    "    max_len = getattr(tokenizer, \"model_max_length\", 4096)\n",
    "\n",
    "    while len(tokens) + reserve_new_tokens > max_len and len(messages) > 2:\n",
    "        messages.pop(1)\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer.encode(text)\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def extract_new_text(full_output, prompt_text):\n",
    "    \"\"\"\n",
    "    Extract only the new assistant reply by removing the entire prompt_text.\n",
    "    This assumes the model may repeat the prompt; we cut everything before the assistant's reply.\n",
    "    \"\"\"\n",
    "    if prompt_text in full_output:\n",
    "        new = full_output[len(prompt_text):].strip()\n",
    "    else:\n",
    "        new = full_output.strip()\n",
    "\n",
    "    # remove any explicit role labels if present\n",
    "    new = new.replace(\"Assistant:\", \"\").replace(\"User:\", \"\")\n",
    "    return new.strip()\n",
    "\n",
    "\n",
    "def typing_effect(text, delay=0.005):\n",
    "    \"\"\"Print the text character by character.\"\"\"\n",
    "    for ch in text:\n",
    "        sys.stdout.write(ch)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(delay)\n",
    "    print()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# INITIAL MESSAGE LIST\n",
    "# -----------------------------\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"you are a helpful and friendly AI assistant that speaks only in English. \"\n",
    "            \"you remember what the user tells you during this session and across sessions using stored memory. \"\n",
    "            \"If the user tells you their name, remember it and use it naturally later. \"\n",
    "            \"Be professional, detailed, and never use any random name unless the user says so. \"\n",
    "            \"Keep responses coherent and consistent with previous messages.\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "if long_memory.get(\"name\"):\n",
    "    messages.append({\"role\": \"system\", \"content\": f\"The user's name is {long_memory['name']}.\"})\n",
    "\n",
    "if long_memory.get(\"facts\"):\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Known facts: \" + \"; \".join(long_memory[\"facts\"])})\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# NOTE: this script ASSUMES `tokenizer` and `model`\n",
    "# are already defined elsewhere (like in your original file).\n",
    "# I intentionally DID NOT add any from_pretrained lines.\n",
    "# ----------------------------------------------------\n",
    "\n",
    "print(\"Chat started! Type 'quit' to exit, or 'save_name:YourName' to save your name.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if user_input.startswith(\"save_name:\"):\n",
    "        name = user_input.split(\":\", 1)[1].strip()\n",
    "        long_memory[\"name\"] = name\n",
    "        MEMORY_FILE.write_text(json.dumps(long_memory, ensure_ascii=False, indent=2))\n",
    "        print(f\"[ Saved your name: {name}]\")\n",
    "        continue\n",
    "\n",
    "    lower_input = user_input.lower()\n",
    "    if \"my name is\" in lower_input:\n",
    "        parts = user_input.split()\n",
    "        if len(parts) >= 4:\n",
    "            name = parts[-1]\n",
    "            long_memory[\"name\"] = name\n",
    "            MEMORY_FILE.write_text(json.dumps(long_memory, ensure_ascii=False, indent=2))\n",
    "            print(f\"[ Saved your name: {name}]\")\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # prune history using your tokenizer (unchanged)\n",
    "    messages = prune_messages(tokenizer, messages, reserve_new_tokens=512)\n",
    "\n",
    "    # create chat template\n",
    "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "    # generate (uses your existing `model`)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.25,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # --------------------------\n",
    "    # REMOVE REPEATED HISTORY (only keep last assistant part if repeated)\n",
    "    # --------------------------\n",
    "    # We operate on a lowercased copy to find the last \"assistant\" marker robustly,\n",
    "    # but re-extract from original generated_text to preserve casing.\n",
    "    lower_generated = generated_text.lower()\n",
    "    if \"assistant\" in lower_generated:\n",
    "        parts = lower_generated.split(\"assistant\")\n",
    "        last_part = parts[-1].strip()\n",
    "        # try to find this last_part in the original generated_text to keep original casing if possible\n",
    "        idx = generated_text.lower().rfind(last_part)\n",
    "        if idx != -1:\n",
    "            generated_text = generated_text[idx:].strip()\n",
    "        else:\n",
    "            # fallback to using the lowercased last part\n",
    "            generated_text = last_part\n",
    "\n",
    "    # clean and extract last reply\n",
    "    response_text = extract_new_text(generated_text, text_input)\n",
    "    response_text = clean_generated(response_text)\n",
    "\n",
    "    typing_effect(response_text)\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "print(\"Chat ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select model type:\n",
      "1. Use LoRA fine-tuned model\n",
      "2. Use Base (original) model\n",
      "Enter 1 or 2: 1\n",
      ">> Loading LoRA fine-tuned model...\n",
      "Retrieving folder contents\n",
      "Processing file 13jVkZtY9-AIu5Wm9CrOApKROCTuWAS5R adapter_config.json\n",
      "Processing file 1AF0J0PkncFLAnqfeROpDEFO6kdnbVDTM adapter_model.safetensors\n",
      "Processing file 14uiKATXyCdwuLUxxLuGzUMjWR0t7D9FT added_tokens.json\n",
      "Processing file 12gMPExwLPcJpID7vCOdA7Tkcl95qxky3 chat_template.jinja\n",
      "Processing file 1VmcoZaB65E4iIqucPRkDZb8kW2Y2yepo merges.txt\n",
      "Processing file 1EnhklNi4W6T6jAjcp6gjs6uvq997KQrF README.md\n",
      "Processing file 1gvAK4AcYOfV19FXmUiHLfzBAOxinsgfj special_tokens_map.json\n",
      "Processing file 1QhsvdoEqqXlPfhHHzyvGhj6AJA0dQFAv tokenizer_config.json\n",
      "Processing file 11sdQDHMdMgnz4dy_LIw2dowYWGrbppzT tokenizer.json\n",
      "Processing file 1_-Ni-zffQLwVwL4ghgFF8Sl8_5m1hAa3 vocab.json\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=13jVkZtY9-AIu5Wm9CrOApKROCTuWAS5R\n",
      "To: /content/lora_model/adapter_config.json\n",
      "100% 1.08k/1.08k [00:00<00:00, 6.68MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1AF0J0PkncFLAnqfeROpDEFO6kdnbVDTM\n",
      "From (redirected): https://drive.google.com/uc?id=1AF0J0PkncFLAnqfeROpDEFO6kdnbVDTM&confirm=t&uuid=d5978f77-4185-4ca2-9462-f661b48b89e0\n",
      "To: /content/lora_model/adapter_model.safetensors\n",
      "100% 264M/264M [00:03<00:00, 70.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14uiKATXyCdwuLUxxLuGzUMjWR0t7D9FT\n",
      "To: /content/lora_model/added_tokens.json\n",
      "100% 707/707 [00:00<00:00, 3.48MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12gMPExwLPcJpID7vCOdA7Tkcl95qxky3\n",
      "To: /content/lora_model/chat_template.jinja\n",
      "100% 4.67k/4.67k [00:00<00:00, 498kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VmcoZaB65E4iIqucPRkDZb8kW2Y2yepo\n",
      "To: /content/lora_model/merges.txt\n",
      "100% 1.67M/1.67M [00:00<00:00, 24.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EnhklNi4W6T6jAjcp6gjs6uvq997KQrF\n",
      "To: /content/lora_model/README.md\n",
      "100% 5.24k/5.24k [00:00<00:00, 20.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1gvAK4AcYOfV19FXmUiHLfzBAOxinsgfj\n",
      "To: /content/lora_model/special_tokens_map.json\n",
      "100% 614/614 [00:00<00:00, 2.85MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1QhsvdoEqqXlPfhHHzyvGhj6AJA0dQFAv\n",
      "To: /content/lora_model/tokenizer_config.json\n",
      "100% 5.43k/5.43k [00:00<00:00, 20.0MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11sdQDHMdMgnz4dy_LIw2dowYWGrbppzT\n",
      "To: /content/lora_model/tokenizer.json\n",
      "100% 11.4M/11.4M [00:00<00:00, 27.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_-Ni-zffQLwVwL4ghgFF8Sl8_5m1hAa3\n",
      "To: /content/lora_model/vocab.json\n",
      "100% 2.78M/2.78M [00:00<00:00, 18.5MB/s]\n",
      "Download completed\n",
      "==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b606fac1dd534cd5b697b9870a285497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34b0017a1ff4e76af5a04b86285ea80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chat started! Type 'quit' to exit, or 'save_name:YourName' to save your name.\n",
      "\n",
      "You: quit\n",
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# the dfference between the base model and lora\n",
    "'''from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, time, sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Select model type:\")\n",
    "print(\"1. Use LoRA fine-tuned model\")\n",
    "print(\"2. Use Base (original) model\")\n",
    "\n",
    "choice = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    print(\">> Loading LoRA fine-tuned model...\")\n",
    "\n",
    "    !gdown --folder https://drive.google.com/drive/u/0/folders/1arl5hzp6La-dibY1kXQ9zPkInraTkPDb -O ./lora_model\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"./lora_model\",\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "elif choice == \"2\":\n",
    "    print(\">> Loading base/original model...\")\n",
    "    BASE_MODEL_NAME = \"unsloth/Qwen2.5-1.5B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "\n",
    "MEMORY_FILE = Path(\"chat_memory.json\")\n",
    "if MEMORY_FILE.exists():\n",
    "    long_memory = json.loads(MEMORY_FILE.read_text())\n",
    "else:\n",
    "    long_memory = {\"name\": None, \"facts\": []}\n",
    "\n",
    "\n",
    "def clean_generated(text: str) -> str:\n",
    "    \"\"\"Remove unwanted tags or prefixes.\"\"\"\n",
    "    text = text.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "    text = text.replace(\"Assistant:\", \"\").replace(\"User:\", \"\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def prune_messages(tokenizer, messages, reserve_new_tokens=512):\n",
    "    \"\"\"Keep conversation within the model's token limit.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    tokens = tokenizer.encode(text)\n",
    "    max_len = getattr(tokenizer, \"model_max_length\", 4096)\n",
    "    while len(tokens) + reserve_new_tokens > max_len and len(messages) > 2:\n",
    "        messages.pop(1)\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        tokens = tokenizer.encode(text)\n",
    "    return messages\n",
    "\n",
    "\n",
    "def extract_new_text(full_output, input_text):\n",
    "    \"\"\"Extract only the new text (the assistant's response).\"\"\"\n",
    "    if input_text in full_output:\n",
    "        return full_output.split(input_text)[-1].strip()\n",
    "    return full_output.strip()\n",
    "\n",
    "\n",
    "def typing_effect(text, delay=0.005):\n",
    "    \"\"\"Print the text character by character.\"\"\"\n",
    "    for ch in text:\n",
    "        sys.stdout.write(ch)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(delay)\n",
    "    print()\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"I'm a helpful and friendly AI assistant that speaks only in English. \"\n",
    "            \"I remember what the user tells you during this session and across sessions using stored memory. \"\n",
    "            \"If the user tells you their name, remember it and use it naturally later. \"\n",
    "            \"Be professional, detailed, and never use any random name like 'Charlie' or 'John' unless the user says so. \"\n",
    "            \"Keep responses coherent and consistent with previous messages.\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "if long_memory.get(\"name\"):\n",
    "    messages.append({\"role\": \"system\", \"content\": f\"The user's name is {long_memory['name']}.\"})\n",
    "if long_memory.get(\"facts\"):\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Known facts: \" + \"; \".join(long_memory[\"facts\"])})\n",
    "\n",
    "print(\"\\n Chat started! Type 'quit' to exit, or 'save_name:YourName' to save your name.\")\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if user_input.startswith(\"save_name:\"):\n",
    "        name = user_input.split(\":\", 1)[1].strip()\n",
    "        long_memory[\"name\"] = name\n",
    "        MEMORY_FILE.write_text(json.dumps(long_memory, ensure_ascii=False, indent=2))\n",
    "        print(f\"[ Saved your name: {name}]\")\n",
    "        continue\n",
    "\n",
    "    lower_input = user_input.lower()\n",
    "    if \"my name is\" in lower_input:\n",
    "        parts = user_input.split()\n",
    "        if len(parts) >= 4:\n",
    "            name = parts[-1]\n",
    "            long_memory[\"name\"] = name\n",
    "            MEMORY_FILE.write_text(json.dumps(long_memory, ensure_ascii=False, indent=2))\n",
    "            print(f\"[ Saved your name: {name}]\")\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    messages = prune_messages(tokenizer, messages, reserve_new_tokens=512)\n",
    "\n",
    "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.25,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_text = extract_new_text(generated_text, text_input)\n",
    "    response_text = clean_generated(response_text)\n",
    "\n",
    "    typing_effect(response_text)\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "print(\"Chat ended.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
